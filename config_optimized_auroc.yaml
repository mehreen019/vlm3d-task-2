# Optimized Multi-Abnormality Classification Configuration for AUROC > 0.6
# VLM3D Task 2: Enhanced training for maximum AUROC performance

# Data configuration
data:
  data_root: "./ct_rate_2d"
  train_csv: "./ct_rate_2d/splits/train_slices.csv"
  val_csv: "./ct_rate_2d/splits/valid_slices.csv"
  test_csv: "./ct_rate_2d/splits/test_slices.csv"

# Model configuration - Optimized for AUROC
model:
  backbone: "efficientnet_b3"  # Higher capacity model
  dropout_rate: 0.5  # High dropout for regularization
  num_classes: 18
  use_attention: "cbam"  # Channel and spatial attention
  use_multiscale_fusion: true
  freeze_backbone_epochs: 10  # Progressive unfreezing

# Training configuration - Optimized for stability and convergence
training:
  batch_size: 12  # Smaller batches for stability
  learning_rate: 2e-5  # Very conservative learning rate
  weight_decay: 2e-4  # Strong regularization
  max_epochs: 200  # More epochs for convergence
  early_stopping_patience: 30  # More patience
  gradient_clip_val: 0.3  # Tight gradient clipping
  accumulate_grad_batches: 3  # Effective batch size = 36
  
  # Advanced training techniques
  use_cosine_annealing: true
  warmup_epochs: 10
  min_lr_ratio: 0.01
  
  # Loss and sampling - Optimized for imbalanced data
  use_focal_loss: true
  focal_alpha: 0.25
  focal_gamma: 3.0  # Higher gamma for hard examples
  use_weighted_sampling: true
  use_mixed_precision: true
  
  # Advanced techniques for AUROC
  use_label_smoothing: true
  label_smoothing_factor: 0.05  # Light smoothing
  use_mixup: true
  mixup_alpha: 0.3
  use_cutmix: true
  cutmix_alpha: 1.0
  cutmix_prob: 0.5
  
  # Hardware
  gpus: 1
  num_workers: 4
  seed: 42
  
  # Directories
  checkpoint_dir: "./checkpoints"
  log_dir: "./logs"

# Class information (18 abnormalities from CT-RATE dataset)
classes:
  - "Atelectasis"
  - "Cardiomegaly" 
  - "Consolidation"
  - "Edema"
  - "Effusion"
  - "Emphysema"
  - "Fibrosis"
  - "Fracture"
  - "Hernia"
  - "Infiltration"
  - "Mass"
  - "Nodule"
  - "Pleural_Thickening"
  - "Pneumonia"
  - "Pneumothorax"
  - "Support_Devices"
  - "Thickening"
  - "No_Finding"

# Evaluation metrics weights - AUROC focused
evaluation:
  metrics:
    auroc_weight: 0.5  # Maximum weight on AUROC
    f1_weight: 0.2
    precision_weight: 0.1
    recall_weight: 0.15
    accuracy_weight: 0.05
  
  # Thresholds optimized for AUROC
  decision_threshold: 0.35  # Lower threshold for better sensitivity
  
  # Cross-validation settings
  cv_folds: 3  # Faster experimentation
  stratified_cv: true

# Aggressive data augmentation for better generalization
augmentation:
  rotation_degrees: 20
  horizontal_flip_prob: 0.7
  vertical_flip_prob: 0.4
  affine_translate: 0.2
  affine_scale: [0.85, 1.15]
  perspective_prob: 0.3
  color_jitter:
    brightness: 0.4
    contrast: 0.4
    saturation: 0.2
    hue: 0.1
  gaussian_noise: 0.03
  gaussian_blur_prob: 0.2
  elastic_transform: true
  grid_distortion_prob: 0.3
  coarse_dropout:
    max_holes: 8
    max_height: 16
    max_width: 16
    fill_value: 0
    prob: 0.5

# Enhanced preprocessing for CT images
preprocessing:
  target_size: [288, 288]  # Larger input for more detail
  normalization:
    mean: [0.485, 0.456, 0.406]
    std: [0.229, 0.224, 0.225]
  
  # CT-specific preprocessing
  hu_windowing:
    min_hu: -1200  # Wider HU range
    max_hu: 600
  clahe: true
  gamma_correction: 1.2
  histogram_equalization: true
  
  # Multi-scale training
  multiscale_training: true
  scale_sizes: [[256, 256], [288, 288], [320, 320]]
  
# Advanced optimization settings
optimization:
  optimizer: "adamw"  # Better than Adam for vision tasks
  scheduler: "cosine_annealing_warm_restarts"
  t_0: 20  # Cosine annealing period
  t_mult: 2  # Period multiplication
  eta_min_ratio: 0.001
  
  # Advanced regularization
  stochastic_weight_averaging: true
  swa_start_epoch: 100
  swa_lr: 1e-5
  
  # Gradient accumulation strategy
  gradient_checkpointing: true  # Memory efficient
  
# Model ensemble (for inference)
ensemble:
  use_tta: true  # Test Time Augmentation
  tta_transforms: 8
  use_model_ensemble: false  # Can be enabled if multiple models trained